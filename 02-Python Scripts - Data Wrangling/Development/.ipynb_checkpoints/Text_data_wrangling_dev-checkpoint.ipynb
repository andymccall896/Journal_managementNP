{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "atmospheric-floating",
   "metadata": {},
   "source": [
    "# Using distance similarity metrics (Unsupervised Machine learning) to optimise pairwise similarities of Scientific journals \n",
    "\n",
    "**summary**: This project will help aid the International Society for knowledge organisation - TSKO - (Singapore Chapter) to better segment scientific journals that are most similar.\n",
    "Trying to explore ML techniques that can perform better than conventional ways to index scientific journals in their respective category.\n",
    "<br>\n",
    "This is the first iteration of the research project to see how successful ML technqiues are against conventional methods. The results of this project will be presented to TSKO in a conference at around September/October in Singapore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interpreted-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling package in Python \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Glob pythons filepath recognition \n",
    "import glob \n",
    "# Pythons package is the way in which it can interact with the operating system\n",
    "import os\n",
    "# natural language processing package \n",
    "import nltk \n",
    "import re\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer   \n",
    "ps = PorterStemmer() \n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-minute",
   "metadata": {},
   "source": [
    "### collating all extracts into one dataframe\n",
    "* Please note that the Lens ID column is the unique identifier for each document which are scientific papers.\n",
    "    * Example Lens ID \n",
    "``` python\n",
    "'116-509-832-503-46X'\n",
    "```\n",
    "* This is important if you need to refer to one document to another in terms of spot checkign and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lens ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date Published</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Publication Type</th>\n",
       "      <th>Source Title</th>\n",
       "      <th>ISSNs</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Source Country</th>\n",
       "      <th>Author/s</th>\n",
       "      <th>...</th>\n",
       "      <th>Source URLs</th>\n",
       "      <th>External URL</th>\n",
       "      <th>PMID</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Microsoft Academic ID</th>\n",
       "      <th>PMCID</th>\n",
       "      <th>Patent Citation Count</th>\n",
       "      <th>References</th>\n",
       "      <th>Scholarly Citation Count</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>000-251-444-646-256</td>\n",
       "      <td>Electronic media and visual knowledge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Knowledge Organization</td>\n",
       "      <td>9437444</td>\n",
       "      <td>International Society for Knowledge Organization</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Kim H. Veltman</td>\n",
       "      <td>...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-1-47 ...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-1-47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5771/0943-7444-1993-1-47</td>\n",
       "      <td>2.495233e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>188-563-904-203-247</td>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>002-768-931-855-543</td>\n",
       "      <td>HYPERICONICS: Hypertext and the social constru...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Knowledge Organization</td>\n",
       "      <td>9437444</td>\n",
       "      <td>International Society for Knowledge Organization</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Gerhard Jan Nauta</td>\n",
       "      <td>...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-1-35 ...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-1-35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5771/0943-7444-1993-1-35</td>\n",
       "      <td>2.505692e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>013-996-415-887-147</td>\n",
       "      <td>Terminological research in the former USSR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Knowledge Organization</td>\n",
       "      <td>9437444</td>\n",
       "      <td>International Society for Knowledge Organization</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Sergey V.Grinev</td>\n",
       "      <td>...</td>\n",
       "      <td>http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=3...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-3-150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5771/0943-7444-1993-3-150</td>\n",
       "      <td>2.475052e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>017-489-591-364-184</td>\n",
       "      <td>Seven fundamental questions for the science of...</td>\n",
       "      <td>1993-12-01</td>\n",
       "      <td>1993</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Knowledge Organization</td>\n",
       "      <td>9437444</td>\n",
       "      <td>International Society for Knowledge Organization</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Robert M. Losee</td>\n",
       "      <td>...</td>\n",
       "      <td>https://uncch.pure.elsevier.com/en/publication...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-2-65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5771/0943-7444-1993-2-65</td>\n",
       "      <td>5.290351e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>005-255-725-462-840; 010-656-047-738-040; 016-...</td>\n",
       "      <td>10</td>\n",
       "      <td>C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>025-077-447-234-296</td>\n",
       "      <td>Subject authory control in a union catalogue: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993</td>\n",
       "      <td>journal article</td>\n",
       "      <td>Knowledge Organization</td>\n",
       "      <td>9437444</td>\n",
       "      <td>International Society for Knowledge Organization</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Maria Ines Lopes</td>\n",
       "      <td>...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-3-133...</td>\n",
       "      <td>http://dx.doi.org/10.5771/0943-7444-1993-3-133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5771/0943-7444-1993-3-133</td>\n",
       "      <td>2.469551e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Lens ID                                              Title  \\\n",
       "621  000-251-444-646-256              Electronic media and visual knowledge   \n",
       "623  002-768-931-855-543  HYPERICONICS: Hypertext and the social constru...   \n",
       "627  013-996-415-887-147         Terminological research in the former USSR   \n",
       "631  017-489-591-364-184  Seven fundamental questions for the science of...   \n",
       "638  025-077-447-234-296  Subject authory control in a union catalogue: ...   \n",
       "\n",
       "    Date Published  Publication Year Publication Type            Source Title  \\\n",
       "621            NaN              1993  journal article  Knowledge Organization   \n",
       "623            NaN              1993  journal article  Knowledge Organization   \n",
       "627            NaN              1993  journal article  Knowledge Organization   \n",
       "631     1993-12-01              1993  journal article  Knowledge Organization   \n",
       "638            NaN              1993  journal article  Knowledge Organization   \n",
       "\n",
       "       ISSNs                                         Publisher Source Country  \\\n",
       "621  9437444  International Society for Knowledge Organization        Germany   \n",
       "623  9437444  International Society for Knowledge Organization        Germany   \n",
       "627  9437444  International Society for Knowledge Organization        Germany   \n",
       "631  9437444  International Society for Knowledge Organization        Germany   \n",
       "638  9437444  International Society for Knowledge Organization        Germany   \n",
       "\n",
       "              Author/s  ...  \\\n",
       "621     Kim H. Veltman  ...   \n",
       "623  Gerhard Jan Nauta  ...   \n",
       "627    Sergey V.Grinev  ...   \n",
       "631    Robert M. Losee  ...   \n",
       "638   Maria Ines Lopes  ...   \n",
       "\n",
       "                                           Source URLs  \\\n",
       "621  http://dx.doi.org/10.5771/0943-7444-1993-1-47 ...   \n",
       "623  http://dx.doi.org/10.5771/0943-7444-1993-1-35 ...   \n",
       "627  http://cat.inist.fr/?aModele=afficheN&cpsidt=3...   \n",
       "631  https://uncch.pure.elsevier.com/en/publication...   \n",
       "638  http://dx.doi.org/10.5771/0943-7444-1993-3-133...   \n",
       "\n",
       "                                       External URL PMID  \\\n",
       "621   http://dx.doi.org/10.5771/0943-7444-1993-1-47  NaN   \n",
       "623   http://dx.doi.org/10.5771/0943-7444-1993-1-35  NaN   \n",
       "627  http://dx.doi.org/10.5771/0943-7444-1993-3-150  NaN   \n",
       "631   http://dx.doi.org/10.5771/0943-7444-1993-2-65  NaN   \n",
       "638  http://dx.doi.org/10.5771/0943-7444-1993-3-133  NaN   \n",
       "\n",
       "                              DOI Microsoft Academic ID PMCID  \\\n",
       "621   10.5771/0943-7444-1993-1-47          2.495233e+09   NaN   \n",
       "623   10.5771/0943-7444-1993-1-35          2.505692e+09   NaN   \n",
       "627  10.5771/0943-7444-1993-3-150          2.475052e+09   NaN   \n",
       "631   10.5771/0943-7444-1993-2-65          5.290351e+07   NaN   \n",
       "638  10.5771/0943-7444-1993-3-133          2.469551e+09   NaN   \n",
       "\n",
       "     Patent Citation Count                                         References  \\\n",
       "621                      0                                188-563-904-203-247   \n",
       "623                      0                                                NaN   \n",
       "627                      0                                                NaN   \n",
       "631                      0  005-255-725-462-840; 010-656-047-738-040; 016-...   \n",
       "638                      0                                                NaN   \n",
       "\n",
       "     Scholarly Citation Count  \\\n",
       "621                         1   \n",
       "623                         0   \n",
       "627                         0   \n",
       "631                        10   \n",
       "638                         0   \n",
       "\n",
       "                                             file_name  \n",
       "621  C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...  \n",
       "623  C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...  \n",
       "627  C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...  \n",
       "631  C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...  \n",
       "638  C:\\Users\\andym\\Desktop\\Datafancy\\Journal_manag...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Concatenate the excel files together\n",
    "print(glob.glob(\"/home/adam/*.txt\"))\n",
    "os.chdir(\"C:/Users/andym/Desktop/Datafancy/Journal_managementNP/01-Raw_data/all_data\")\n",
    "# get data file names\n",
    "path =r'C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\01-Raw_data\\all_data'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "### set an empty list?\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['file_name'] = filename\n",
    "    li.append(df)\n",
    "    \n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "dframe = pd.DataFrame(data = frame)\n",
    "dframe['Publication Year'] = dframe['Publication Year'].astype('int') \n",
    "dframetrim = dframe[dframe.Abstract.notnull()]\n",
    "dframetrim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-outreach",
   "metadata": {},
   "source": [
    "### Preprocessing the Abstract column of all the documents\n",
    "1. Remove all special characters like ! , ? and , etc\n",
    "2. Tokenise the entire string into individual elements in a list\n",
    "3. Temove all stop words using the stopwords corpus from the nltk package\n",
    "4. Stemming all words to its root word. For example , theory and theories to just theory and theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thorough-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "englishstopwords = stopwords.words('english')\n",
    "\n",
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract']\n",
    "\n",
    "for i in range(len(dframetrim['Abstract'])):\n",
    "    dframetrim['Abstract_trim_2'].values[i]  = re.sub('[^A-Za-z0-9]+', ' ', dframetrim['Abstract_trim_2'].values[i].lower()) ## Lower case and special characters removed\n",
    "    dframetrim['Abstract_trim_2'].values[i]  = dframetrim['Abstract_trim_2'].values[i].split(' ') ## tokenise\n",
    "##### I might need to look up lambda\n",
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract_trim_2'].apply(lambda x: [item for item in x if item not in stopwords.words('english')]) ## Stopwords\n",
    "dframetrim['Abstract_trim_2'] =  dframetrim['Abstract_trim_2'].apply(lambda x: [porter.stem(item) for item in x ]) ## Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-quest",
   "metadata": {},
   "source": [
    "### Removing other useless words like 'use' or 'www'\n",
    "* for spot checking individual words to be included into the word_remove_list\n",
    "  1. you can create a distinct list of words with its respective count frequency using the counter function from the collections package\n",
    "  2. Manually inspect each word ordered by highest count to include in the word_remove_list list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wound-storm",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "word_remove_list = ['knowledg','classif','inform','use','www','good','','organ'\n",
    "                    ,'paper','subject','term','studi','develop','term','librari'\n",
    "                    ,'differ','index','approach','work','base','scienc','field'\n",
    "                    ,'user','structure','ko','www','present','ko','also','p',\n",
    "                    'two','one','provid','author','need','tag','within','way','well','includ',\n",
    "                   'part','three','may','2','e','ddc','1','3','4','5','6','7','8','9','10','li',\n",
    "                    'would','non','de','four','ir','thu','frbr','737','g',\n",
    "                    'etc','av','br','co','th','o','sg','nr','per','1087',\n",
    "                   'six','j','via','miss','r','et','dan','bia','niac','sch','17525900'\n",
    "                   'u','lcc','c','ii','la','n','dc','aat' ,'24','TRUE','keep','220','le','po','stw',\n",
    "                   'kdc','oto','gsc','gsp','k','pt','dr','go','100','cc','il','nt','280','b','l','v',\n",
    "                   '20','kr','sp','ave','se','12','0','23','33','157','iv','rd','am','mid','34','be','iii','fid'\n",
    "                    ,'lc','met','20th' ,'ste','axi','dsi','axi','200','230','223','hlt','sur','du','mesh'\n",
    "                    ,'lcsh','oclc','eu','atn','2nd','bc2','di','76','500','get','http','sdss','crg'\n",
    "                    ,'esp','sab','lay','11','37','cs','urg','17525000','ot','fo','hn','em','ncft','48',\n",
    "                    '40','168','dec','pc','fix','ccq','lrt','ic','fix','owe','120th','87','510','94','49',\n",
    "                    '6n','mit','rev','int','32','136','144','cr','119','148','wall','120','ckm','cd','rom',\n",
    "                    'ata','nest','sdoc','soli','istei','tct','shl','ana','asp','dk5','240','292','299','f','ég','222'\n",
    "                    ,'229','59','pin','sen','twelv','bbk','14','28','180','192','cle','150','ec','np','ifla','ey','dea',\n",
    "                    'cpcl','nl','601','178','215','iskoi','htm','i','ref','31','92','97','çmf','ibn','vo','inv','un','kop','cen','21st'\n",
    "                    ,'659','ph','229','26','va','oc','ufpr','80060000','ena','od','fu','3d','dog','ear','vi','saa','vi','62',\n",
    "                    'ed','43hing','kno','pubm','39','268','275','els','fac','nd','ek','oth','er','cep','ia','iti','38','114',\n",
    "                   '122','35','lam','liu','ru','16','403','41','311','318','ga','eas','gri','tgn','cut','off','add','lrm','cm','ric','collat',\n",
    "                    'stop','unlik','die','sdi','90','379','1a','1b','rda','riv','hj','pac','hoc','rid','rio','ssn','real',\n",
    "                   'ccp','iht','edm','today','lie','point','bcc']\n",
    "\n",
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract_trim_2'].apply(lambda x: [item for item in x if item not in word_remove_list]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-subcommittee",
   "metadata": {},
   "source": [
    "### Function to create the extract NLP features for the unsupervised machine learning project\n",
    "The function creates multiple sets of individual words as columns by IDF , count(word count field), Cal_Term_Freq_TF and Binary_count thus creating around 13,000 fields!\n",
    "<br>\n",
    "This function creates multiple pivots of distinct words into columns by:\n",
    " 1. Count: the number of times that word has appeared in that individual abstract\n",
    " 2. Binary_count: Binary output of that word actually appearing in that document or not (1 = yes,0 = no)\n",
    " 3. Cal_Term_Freq_TF: count number of times that word has appeared in that abstract divided by the total words in that abstract\n",
    " 4. Cal_Inverse_Document_Freq_IDF: Log(total number of documents in that total dataset / number of times that word was mentioned in any abstract within that total dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coupled-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_df(testing_loop):\n",
    "\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "\n",
    "    def getUniqueWords(allWords):\n",
    "        uniqueWords = [] \n",
    "        for i in allWords:\n",
    "            if not i in uniqueWords:\n",
    "                uniqueWords.append(i)\n",
    "        return uniqueWords\n",
    "\n",
    "    testing_loop_long = testing_loop\n",
    "    testing_loop_long['Abstract_trim_2_distinct'] = testing_loop_long['Abstract_trim_2']\n",
    "    for id,i in enumerate(testing_loop_long['Abstract_trim_2']):\n",
    "        testing_loop_long['Abstract_trim_2_distinct'].iloc[id] = getUniqueWords(i)\n",
    "        \n",
    "    testing_loop_long = testing_loop_long[testing_loop_long.astype(str)['Abstract_trim_2_distinct'] != '[]']\n",
    "    testing_loop = testing_loop[testing_loop.astype(str)['Abstract_trim_2_distinct'] != '[]']    \n",
    "\n",
    "    testing_loop['abstract_counts'] = [Counter(x).most_common() for x in testing_loop['Abstract_trim_2']]\n",
    "\n",
    "    testing_loop = testing_loop[testing_loop.astype(str)['abstract_counts'] != '[]']\n",
    "\n",
    "    testing_loop_long = testing_loop[['Lens ID','Abstract_trim_2_distinct','Abstract_trim_2']].explode('Abstract_trim_2_distinct')\n",
    "\n",
    "    testing_loop_long['abstract_counts_unlist'] = [val for sublist in testing_loop['abstract_counts'] for val in sublist]\n",
    "\n",
    "#pd.DataFrame(testing_loop['abstract_counts_unlist'])\n",
    "#### Unzip the individual tuples and covert into seperate columns\n",
    "#testing_loop_long['Word'] = testing_loop_long['Abstract_trim_2_distinct']\n",
    "#testing_loop_long['Count'] = testing_loop_long['Abstract_trim_2_distinct']\n",
    "    testing_loop_long['Word'], testing_loop_long['Count'] = zip(*testing_loop_long['abstract_counts_unlist'])\n",
    "    testing_loop_long['Binary_count'] = 1\n",
    "\n",
    "    testing_loop_long[\"Total_words_in_abstract\"] = testing_loop_long['Abstract_trim_2']\n",
    "    for id,i in enumerate(testing_loop_long['Abstract_trim_2']):\n",
    "        testing_loop_long[\"Total_words_in_abstract\"].iloc[id] = len(testing_loop_long['Abstract_trim_2'].iloc[id])\n",
    "    testing_loop_long['Cal_Term_Freq_TF'] =  (testing_loop_long['Count']/testing_loop_long[\"Total_words_in_abstract\"])\n",
    "    \n",
    "    testing_loop_long_Count = testing_loop_long[['Lens ID','Word','Count']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Count').fillna(0).add_prefix('Count_')   \n",
    "    testing_loop_long_Count = pd.DataFrame(testing_loop_long_Count.to_records())\n",
    "\n",
    "    testing_loop_long_TF = testing_loop_long[['Lens ID','Word','Cal_Term_Freq_TF']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Cal_Term_Freq_TF',aggfunc = np.sum).fillna(0).add_prefix('TF_')      \n",
    "    testing_loop_long_TF = pd.DataFrame(testing_loop_long_TF.to_records()) \n",
    "\n",
    "    testing_loop_long_Binary = testing_loop_long[['Lens ID','Word','Binary_count']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Binary_count').fillna(0).add_prefix('Binary_') \n",
    "    testing_loop_long_Binary = pd.DataFrame(testing_loop_long_Binary.to_records()) \n",
    "\n",
    "    testing_loop_long['abstract_concat_list'] = testing_loop_long['Abstract_trim_2']\n",
    "    for i in range(len(testing_loop_long['Abstract_trim_2'])):\n",
    "        testing_loop_long['abstract_concat_list'].values[i] = list(testing_loop['Abstract_trim_2'])\n",
    "    \n",
    "    testing_loop_long['dsad'] = testing_loop_long['abstract_concat_list']\n",
    "    for id1,o in enumerate(testing_loop_long['abstract_concat_list']):\n",
    "        for id2,i in enumerate(o):\n",
    "            testing_loop_long['dsad'].iloc[id1][id2] = [i for i in testing_loop_long['abstract_concat_list'].iloc[id1][id2] if i == testing_loop_long['Word'].values[id1]] \n",
    "\n",
    "    for id,i in enumerate(testing_loop_long['dsad']):\n",
    "        testing_loop_long['dsad'].values[id] = [x for x in testing_loop_long['dsad'].values[id] if x != []]        \n",
    "\n",
    "    testing_loop_long['times_mentioned_in_abstracts'] = [len(x) for x in testing_loop_long['dsad']]   \n",
    "    testing_loop_long['Total_number_abstracts'] = len(testing_loop[\"Abstract_trim_2\"])       \n",
    "    testing_loop_long['Cal_Inverse_Document_Freq_IDF'] = np.log10(testing_loop_long['Total_number_abstracts']/testing_loop_long['times_mentioned_in_abstracts'])\n",
    "\n",
    "    testing_loop_long_IDF = testing_loop_long[['Lens ID','Word','Cal_Inverse_Document_Freq_IDF']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Cal_Inverse_Document_Freq_IDF').fillna(0).add_prefix('IDF_')  \n",
    "    testing_loop_long_IDF = pd.DataFrame(testing_loop_long_IDF.to_records()) \n",
    "\n",
    "    testing_loop_wide = testing_loop_long_IDF.set_index('Lens ID').join(testing_loop_long_TF.set_index('Lens ID')).join(testing_loop_long_Count.set_index('Lens ID')).join(testing_loop_long_Binary.set_index('Lens ID'))\n",
    "\n",
    "    return testing_loop_wide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-austria",
   "metadata": {},
   "source": [
    "#### After applying the function for data feature engineering \n",
    "We have the following dataset below/\n",
    "You can run the function by uncommenting the line in the code chunk  \n",
    "\n",
    "```python \n",
    "test = word_count_df(testing_loop =  dframetrim)\n",
    "```\n",
    "However, this function is quite computational taxing and will read the csv file instead\n",
    "<br>\n",
    "\n",
    "The reason why I created it into a function is because I can do the computation on a small subset as shown below in the code chunk\n",
    "\n",
    "```python \n",
    "testsmall = word_count_df(testing_loop =  dframetrim.iloc[0:2])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stupid-dispatch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF_000</th>\n",
       "      <th>IDF_01</th>\n",
       "      <th>IDF_113</th>\n",
       "      <th>IDF_1232</th>\n",
       "      <th>IDF_1274</th>\n",
       "      <th>IDF_1294</th>\n",
       "      <th>IDF_1296</th>\n",
       "      <th>IDF_13</th>\n",
       "      <th>IDF_1316</th>\n",
       "      <th>IDF_1700</th>\n",
       "      <th>...</th>\n",
       "      <th>Binary_yellow</th>\n",
       "      <th>Binary_yet</th>\n",
       "      <th>Binary_yield</th>\n",
       "      <th>Binary_yoga</th>\n",
       "      <th>Binary_yogic</th>\n",
       "      <th>Binary_zation</th>\n",
       "      <th>Binary_zenon</th>\n",
       "      <th>Binary_zentralblatt</th>\n",
       "      <th>Binary_zone</th>\n",
       "      <th>Binary_zoolog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lens ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>000-189-988-224-76X</th>\n",
       "      <td>2.251638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000-251-444-646-256</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001-254-322-670-872</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001-281-947-751-975</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>001-542-585-753-06X</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.552668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13276 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IDF_000  IDF_01  IDF_113  IDF_1232  IDF_1274  IDF_1294  \\\n",
       "Lens ID                                                                        \n",
       "000-189-988-224-76X  2.251638     0.0      0.0       0.0       0.0       0.0   \n",
       "000-251-444-646-256  0.000000     0.0      0.0       0.0       0.0       0.0   \n",
       "001-254-322-670-872  0.000000     0.0      0.0       0.0       0.0       0.0   \n",
       "001-281-947-751-975  0.000000     0.0      0.0       0.0       0.0       0.0   \n",
       "001-542-585-753-06X  0.000000     0.0      0.0       0.0       0.0       0.0   \n",
       "\n",
       "                     IDF_1296    IDF_13  IDF_1316  IDF_1700  ...  \\\n",
       "Lens ID                                                      ...   \n",
       "000-189-988-224-76X       0.0  0.000000       0.0       0.0  ...   \n",
       "000-251-444-646-256       0.0  0.000000       0.0       0.0  ...   \n",
       "001-254-322-670-872       0.0  0.000000       0.0       0.0  ...   \n",
       "001-281-947-751-975       0.0  0.000000       0.0       0.0  ...   \n",
       "001-542-585-753-06X       0.0  2.552668       0.0       0.0  ...   \n",
       "\n",
       "                     Binary_yellow  Binary_yet  Binary_yield  Binary_yoga  \\\n",
       "Lens ID                                                                     \n",
       "000-189-988-224-76X            0.0         0.0           0.0          0.0   \n",
       "000-251-444-646-256            0.0         0.0           0.0          0.0   \n",
       "001-254-322-670-872            0.0         0.0           0.0          0.0   \n",
       "001-281-947-751-975            0.0         0.0           0.0          0.0   \n",
       "001-542-585-753-06X            0.0         0.0           0.0          0.0   \n",
       "\n",
       "                     Binary_yogic  Binary_zation  Binary_zenon  \\\n",
       "Lens ID                                                          \n",
       "000-189-988-224-76X           0.0            0.0           0.0   \n",
       "000-251-444-646-256           0.0            0.0           0.0   \n",
       "001-254-322-670-872           0.0            0.0           0.0   \n",
       "001-281-947-751-975           0.0            0.0           0.0   \n",
       "001-542-585-753-06X           0.0            0.0           0.0   \n",
       "\n",
       "                     Binary_zentralblatt  Binary_zone  Binary_zoolog  \n",
       "Lens ID                                                               \n",
       "000-189-988-224-76X                  0.0          0.0            0.0  \n",
       "000-251-444-646-256                  0.0          0.0            0.0  \n",
       "001-254-322-670-872                  0.0          0.0            0.0  \n",
       "001-281-947-751-975                  0.0          0.0            0.0  \n",
       "001-542-585-753-06X                  0.0          0.0            0.0  \n",
       "\n",
       "[5 rows x 13276 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## If you need to re-create the word_count_df\n",
    "#testsmall = word_count_df(testing_loop =  dframetrim.iloc[0:2])\n",
    "#test = word_count_df(testing_loop =  dframetrim)\n",
    "#test.to_csv(r'C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\Final_pivot_dataset_abstract.csv')\n",
    "\n",
    "Final_pivot_dataset = pd.read_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\Final_pivot_dataset_abstract.csv\")\n",
    "Final_pivot_dataset = Final_pivot_dataset.set_index(\"Lens ID\")\n",
    "Final_pivot_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-processor",
   "metadata": {},
   "source": [
    "### Using distance based similarity metrics to find optimal pairwise similarities of scientific journals (unsupervised machine learning)\n",
    "* Datasets was randomly split between train (285 rows or 80% of total dataset) and test sets (72 rows or 20% of total dataset)\n",
    "* Only selecting IDF columns for this analysis\n",
    "* Using sklearn package to retrieve the libary of different distance measures\n",
    "   1. Cosine similarity\n",
    "   2. Hamming Distance\n",
    "   3. Jaccard \n",
    "* Ended up using the cosine similarity metric since it's caculation is robust with a very sparse dataset (i.e a wide dataset/highly dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "boring-transcript",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Lens ID</th>\n",
       "      <th>097-421-372-053-520</th>\n",
       "      <th>116-509-832-503-46X</th>\n",
       "      <th>043-454-317-567-459</th>\n",
       "      <th>036-149-448-029-082</th>\n",
       "      <th>062-055-064-561-85X</th>\n",
       "      <th>061-018-838-320-648</th>\n",
       "      <th>179-132-294-510-213</th>\n",
       "      <th>078-881-949-441-033</th>\n",
       "      <th>116-085-787-369-516</th>\n",
       "      <th>046-376-269-492-795</th>\n",
       "      <th>...</th>\n",
       "      <th>098-828-423-432-071</th>\n",
       "      <th>067-137-702-908-40X</th>\n",
       "      <th>106-561-636-619-444</th>\n",
       "      <th>099-563-150-734-423</th>\n",
       "      <th>012-652-438-359-587</th>\n",
       "      <th>188-192-965-023-127</th>\n",
       "      <th>111-093-646-386-599</th>\n",
       "      <th>144-109-666-243-086</th>\n",
       "      <th>121-066-553-719-090</th>\n",
       "      <th>092-507-541-687-774</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lens ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122-703-378-850-053</th>\n",
       "      <td>0.046075</td>\n",
       "      <td>0.013647</td>\n",
       "      <td>0.012873</td>\n",
       "      <td>0.051560</td>\n",
       "      <td>0.054563</td>\n",
       "      <td>0.050737</td>\n",
       "      <td>0.062397</td>\n",
       "      <td>0.049819</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>0.092991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020608</td>\n",
       "      <td>0.055795</td>\n",
       "      <td>0.051258</td>\n",
       "      <td>0.065044</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.035662</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.049747</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>0.020763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>024-276-265-590-973</th>\n",
       "      <td>0.040656</td>\n",
       "      <td>0.113683</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.014896</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.057319</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>0.091528</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.051186</td>\n",
       "      <td>0.015136</td>\n",
       "      <td>0.031840</td>\n",
       "      <td>0.024567</td>\n",
       "      <td>0.052523</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.016853</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.024493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191-349-254-093-883</th>\n",
       "      <td>0.035778</td>\n",
       "      <td>0.040962</td>\n",
       "      <td>0.034939</td>\n",
       "      <td>0.017814</td>\n",
       "      <td>0.066108</td>\n",
       "      <td>0.031187</td>\n",
       "      <td>0.076319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033168</td>\n",
       "      <td>0.039808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024243</td>\n",
       "      <td>0.050914</td>\n",
       "      <td>0.035137</td>\n",
       "      <td>0.056583</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.040904</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.039702</td>\n",
       "      <td>0.026393</td>\n",
       "      <td>0.035599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185-129-272-793-400</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030327</td>\n",
       "      <td>0.036516</td>\n",
       "      <td>0.029594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056704</td>\n",
       "      <td>0.011957</td>\n",
       "      <td>0.021959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.01392</td>\n",
       "      <td>0.012130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120-767-088-360-769</th>\n",
       "      <td>0.072148</td>\n",
       "      <td>0.077910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021129</td>\n",
       "      <td>0.031473</td>\n",
       "      <td>0.022061</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081118</td>\n",
       "      <td>0.013313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023231</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.041261</td>\n",
       "      <td>0.027389</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>0.039887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Lens ID              097-421-372-053-520  116-509-832-503-46X  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.046075             0.013647   \n",
       "024-276-265-590-973             0.040656             0.113683   \n",
       "191-349-254-093-883             0.035778             0.040962   \n",
       "185-129-272-793-400             0.000000             0.035100   \n",
       "120-767-088-360-769             0.072148             0.077910   \n",
       "\n",
       "Lens ID              043-454-317-567-459  036-149-448-029-082  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.012873             0.051560   \n",
       "024-276-265-590-973             0.012610             0.004603   \n",
       "191-349-254-093-883             0.034939             0.017814   \n",
       "185-129-272-793-400             0.000000             0.030327   \n",
       "120-767-088-360-769             0.000000             0.021129   \n",
       "\n",
       "Lens ID              062-055-064-561-85X  061-018-838-320-648  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.054563             0.050737   \n",
       "024-276-265-590-973             0.014896             0.033789   \n",
       "191-349-254-093-883             0.066108             0.031187   \n",
       "185-129-272-793-400             0.036516             0.029594   \n",
       "120-767-088-360-769             0.031473             0.022061   \n",
       "\n",
       "Lens ID              179-132-294-510-213  078-881-949-441-033  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.062397             0.049819   \n",
       "024-276-265-590-973             0.057319             0.009548   \n",
       "191-349-254-093-883             0.076319             0.000000   \n",
       "185-129-272-793-400             0.000000             0.000000   \n",
       "120-767-088-360-769             0.007320             0.000000   \n",
       "\n",
       "Lens ID              116-085-787-369-516  046-376-269-492-795  ...  \\\n",
       "Lens ID                                                        ...   \n",
       "122-703-378-850-053             0.032773             0.092991  ...   \n",
       "024-276-265-590-973             0.091528             0.012198  ...   \n",
       "191-349-254-093-883             0.033168             0.039808  ...   \n",
       "185-129-272-793-400             0.012585             0.011455  ...   \n",
       "120-767-088-360-769             0.081118             0.013313  ...   \n",
       "\n",
       "Lens ID              098-828-423-432-071  067-137-702-908-40X  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.020608             0.055795   \n",
       "024-276-265-590-973             0.018355             0.051186   \n",
       "191-349-254-093-883             0.024243             0.050914   \n",
       "185-129-272-793-400             0.000000             0.056704   \n",
       "120-767-088-360-769             0.023231             0.001012   \n",
       "\n",
       "Lens ID              106-561-636-619-444  099-563-150-734-423  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.051258             0.065044   \n",
       "024-276-265-590-973             0.015136             0.031840   \n",
       "191-349-254-093-883             0.035137             0.056583   \n",
       "185-129-272-793-400             0.011957             0.021959   \n",
       "120-767-088-360-769             0.041261             0.027389   \n",
       "\n",
       "Lens ID              012-652-438-359-587  188-192-965-023-127  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053             0.003861             0.035662   \n",
       "024-276-265-590-973             0.024567             0.052523   \n",
       "191-349-254-093-883             0.005999             0.040904   \n",
       "185-129-272-793-400             0.000000             0.044913   \n",
       "120-767-088-360-769             0.056537             0.039887   \n",
       "\n",
       "Lens ID              111-093-646-386-599  144-109-666-243-086  \\\n",
       "Lens ID                                                         \n",
       "122-703-378-850-053              0.00000             0.049747   \n",
       "024-276-265-590-973              0.00000             0.016853   \n",
       "191-349-254-093-883              0.00000             0.039702   \n",
       "185-129-272-793-400              0.01392             0.012130   \n",
       "120-767-088-360-769              0.00000             0.030539   \n",
       "\n",
       "Lens ID              121-066-553-719-090  092-507-541-687-774  \n",
       "Lens ID                                                        \n",
       "122-703-378-850-053             0.018606             0.020763  \n",
       "024-276-265-590-973             0.025999             0.024493  \n",
       "191-349-254-093-883             0.026393             0.035599  \n",
       "185-129-272-793-400             0.000000             0.008138  \n",
       "120-767-088-360-769             0.000000             0.027998  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Split data sets randomly from 80% to 20% and do a iterative loop to of comparing each \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Final_pivot_dataset_IDF = Final_pivot_dataset.loc[:, Final_pivot_dataset.columns.str.startswith(\"IDF_\")]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "##### set seeds to control the randomness \n",
    "x_train ,x_test = train_test_split(Final_pivot_dataset_IDF,test_size=0.2,random_state  = 11) \n",
    "x_test\n",
    "\n",
    "###########################################################################################\n",
    "#################### COSINE SIMILARITY ####################################################\n",
    "###########################################################################################\n",
    "\n",
    "#### Needs spot checking\n",
    "test_matrix_cosine = cosine_similarity(x_train,x_test) ### I think this does work\n",
    "test_matrix_cosine.shape\n",
    "\n",
    "#pd.DataFrame(test_matrix_cosine).to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix.csv\")\n",
    "test_matrix_cosine_df = pd.DataFrame(test_matrix_cosine)\n",
    "test_matrix_cosine_df = test_matrix_cosine_df.set_index(x_train.index)\n",
    "test_matrix_cosine_df.columns = x_test.index\n",
    "test_matrix_cosine_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_cosine.csv\")\n",
    "\n",
    "############################################################################################\n",
    "#################### HAMMING DISTANCE ######################################################\n",
    "############################################################################################\n",
    "from scipy.spatial import distance\n",
    "\n",
    "lst2 = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    lst2.append(i)\n",
    "\n",
    "lst = [[] for _ in range(len(x_train))]\n",
    "test_matrix_hamming =[lst2 for i in lst]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_test)):\n",
    "        test_matrix_hamming[i][j] = distance.hamming(x_train[i:i+1],x_test[j:j+1])        \n",
    "        \n",
    "test_matrix_hamming_df = pd.DataFrame(test_matrix_hamming)\n",
    "test_matrix_hamming_df = test_matrix_hamming_df.set_index(x_train.index)\n",
    "test_matrix_hamming_df.columns = x_test.index\n",
    "test_matrix_hamming_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_hamming.csv\")\n",
    "############################################################################################\n",
    "#################### JACCARD DISTANCE ######################################################\n",
    "############################################################################################\n",
    "from scipy.spatial import distance\n",
    "distance.jaccard\n",
    "\n",
    "lst2 = []\n",
    "for i in range(len(x_test)):\n",
    "    lst2.append(i)\n",
    "\n",
    "lst = [[] for _ in range(len(x_train))]\n",
    "test_matrix_jaccard =[lst2 for i in lst]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_test)):\n",
    "        test_matrix_jaccard[i][j] = distance.jaccard(x_train[i:i+1],x_test[j:j+1])    \n",
    "                \n",
    "test_matrix_jaccard_df = pd.DataFrame(test_matrix_jaccard)\n",
    "test_matrix_jaccard_df = test_matrix_jaccard_df.set_index(x_train.index)\n",
    "test_matrix_jaccard_df.columns = x_test.index\n",
    "test_matrix_jaccard_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_jaccard.csv\")\n",
    "\n",
    "test_matrix_cosine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-midwest",
   "metadata": {},
   "source": [
    "### Spots check below to see if the Cosine similarity matrix calculations are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "metropolitan-paint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01364743]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### spot check each Similarity matrix\n",
    "x_train.columns\n",
    "x_test.columns\n",
    "x_train.index\n",
    "#103-364-820-610-122\n",
    "#129-504-756-103-235\n",
    "x_test.index\n",
    "#000-251-444-646-256\n",
    "#114-672-389-593-030\n",
    "\n",
    "###### Cosine Similarity test \n",
    "test_matrix_cosine_df\n",
    "cosine_similarity(x_train[x_train.index =='122-703-378-850-053'],x_test[x_test.index =='116-509-832-503-46X'])\n",
    "#cosine_similarity(x_train[x_train.index =='129-504-756-103-235'],x_test[x_test.index =='166-898-814-966-01X'])\n",
    "#cosine_similarity(x_train[x_train.index =='129-504-756-103-235'],x_test[x_test.index =='166-898-814-966-01X'])\n",
    "#cosine_similarity(x_train[x_train.index =='048-107-503-236-974'],x_test[x_test.index =='001-281-947-751-975'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-halloween",
   "metadata": {},
   "source": [
    "### Creating a matrix that collapses the cosine similarity matrix with highest pairwise similarity score\n",
    "* by column it will find the document row with the highest simialrity score for each column\n",
    "* collapsing to a new matrix by rank (1 being the highest similarity score for that pairwise) to 73 rows to 73 columns (column 1 and row 1 being the Lens ID value)\n",
    "* 72 best pairwise match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "vocational-climb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Lens ID</th>\n",
       "      <th>Lens ID</th>\n",
       "      <th>097-421-372-053-520</th>\n",
       "      <th>116-509-832-503-46X</th>\n",
       "      <th>043-454-317-567-459</th>\n",
       "      <th>036-149-448-029-082</th>\n",
       "      <th>062-055-064-561-85X</th>\n",
       "      <th>061-018-838-320-648</th>\n",
       "      <th>179-132-294-510-213</th>\n",
       "      <th>078-881-949-441-033</th>\n",
       "      <th>116-085-787-369-516</th>\n",
       "      <th>...</th>\n",
       "      <th>098-828-423-432-071</th>\n",
       "      <th>067-137-702-908-40X</th>\n",
       "      <th>106-561-636-619-444</th>\n",
       "      <th>099-563-150-734-423</th>\n",
       "      <th>012-652-438-359-587</th>\n",
       "      <th>188-192-965-023-127</th>\n",
       "      <th>111-093-646-386-599</th>\n",
       "      <th>144-109-666-243-086</th>\n",
       "      <th>121-066-553-719-090</th>\n",
       "      <th>092-507-541-687-774</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lens ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>078-470-148-980-604</th>\n",
       "      <td>078-470-148-980-604</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>024-276-265-590-973</th>\n",
       "      <td>024-276-265-590-973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>056-843-928-028-56X</th>\n",
       "      <td>056-843-928-028-56X</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>036-074-742-305-080</th>\n",
       "      <td>036-074-742-305-080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>064-751-648-054-001</th>\n",
       "      <td>064-751-648-054-001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Lens ID                          Lens ID  097-421-372-053-520  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604  078-470-148-980-604                  1.0   \n",
       "024-276-265-590-973  024-276-265-590-973                  0.0   \n",
       "056-843-928-028-56X  056-843-928-028-56X                  0.0   \n",
       "036-074-742-305-080  036-074-742-305-080                  0.0   \n",
       "064-751-648-054-001  064-751-648-054-001                  0.0   \n",
       "\n",
       "Lens ID              116-509-832-503-46X  043-454-317-567-459  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  1.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  1.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              036-149-448-029-082  062-055-064-561-85X  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  1.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  1.0   \n",
       "\n",
       "Lens ID              061-018-838-320-648  179-132-294-510-213  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              078-881-949-441-033  116-085-787-369-516  ...  \\\n",
       "Lens ID                                                        ...   \n",
       "078-470-148-980-604                  0.0                  0.0  ...   \n",
       "024-276-265-590-973                  0.0                  0.0  ...   \n",
       "056-843-928-028-56X                  0.0                  0.0  ...   \n",
       "036-074-742-305-080                  0.0                  0.0  ...   \n",
       "064-751-648-054-001                  0.0                  0.0  ...   \n",
       "\n",
       "Lens ID              098-828-423-432-071  067-137-702-908-40X  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              106-561-636-619-444  099-563-150-734-423  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              012-652-438-359-587  188-192-965-023-127  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              111-093-646-386-599  144-109-666-243-086  \\\n",
       "Lens ID                                                         \n",
       "078-470-148-980-604                  0.0                  0.0   \n",
       "024-276-265-590-973                  0.0                  0.0   \n",
       "056-843-928-028-56X                  0.0                  0.0   \n",
       "036-074-742-305-080                  0.0                  0.0   \n",
       "064-751-648-054-001                  0.0                  0.0   \n",
       "\n",
       "Lens ID              121-066-553-719-090  092-507-541-687-774  \n",
       "Lens ID                                                        \n",
       "078-470-148-980-604                  0.0                  0.0  \n",
       "024-276-265-590-973                  0.0                  0.0  \n",
       "056-843-928-028-56X                  0.0                  0.0  \n",
       "036-074-742-305-080                  0.0                  0.0  \n",
       "064-751-648-054-001                  0.0                  0.0  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Trying to find the the maximum simialrity by the test columns\n",
    "#### Using windows analytics functions to retrieve the pair that gives the maximum cosine similarity\n",
    "#### For filtering rows and columns more effectively\n",
    "####https://towardsdatascience.com/effective-data-filtering-in-pandas-using-loc-40eb815455b6\n",
    "#### Will be using windows analytics function and a for loop to get a rank of highest similarity \n",
    "#### Will need to do a full loop \n",
    "import numpy as np\n",
    "rank_matrix_cosine_df = pd.DataFrame()\n",
    "test_matrix_cosine_df['Lens ID'] = test_matrix_cosine_df.index\n",
    "rank_matrix_cosine_df['Lens ID'] = test_matrix_cosine_df.index\n",
    "test_columns = test_matrix_cosine_df.columns[test_matrix_cosine_df.columns != 'Lens ID']\n",
    "\n",
    "##########################################################################################\n",
    "#for cols in test_columns: \n",
    "#    rank_matrix_cosine_df = pd.DataFrame(test_matrix_cosine_df[test_columns].rank(method = 'first', ascending = False))\n",
    "\n",
    "#### The list comprehension way!! Probs not apropriate \n",
    "rank_matrix_cosine_df = [pd.DataFrame(test_matrix_cosine_df[test_columns].rank(method = 'first', ascending = False)) for i in test_columns][1] \n",
    "#rank_matrix_cosine_df.columns = 'Rank_' + rank_matrix_cosine_df.columns.values\n",
    "\n",
    "##### get pairwise with highest similarity score\n",
    "##### 1 is for best pairwise match based on similarity score or otherwise 0\n",
    "\n",
    "##### Cosine similarity rank matrix\n",
    "lst = []\n",
    "rank_cols = rank_matrix_cosine_df.columns\n",
    "\n",
    "rank_matrix_cosine_df['Lens ID'] = test_matrix_cosine_df.index\n",
    "\n",
    "for cols in rank_cols:\n",
    "    lst.append(rank_matrix_cosine_df[['Lens ID',cols]].loc[rank_matrix_cosine_df[cols] == 1])\n",
    "    \n",
    "rank_matrix_cosine_df_final = pd.concat(lst).fillna(0)\n",
    "rank_matrix_cosine_df_final.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\rank_matrix_cosine_df_final.csv\")\n",
    "\n",
    "########################################################\n",
    "######################### Hamming ######################\n",
    "\n",
    "rank_matrix_hamming_df = pd.DataFrame()\n",
    "rank_matrix_hamming_df['Lens ID'] = test_matrix_hamming_df.index\n",
    "rank_matrix_hamming_df.index = test_matrix_hamming_df.index\n",
    "test_columns = test_matrix_hamming_df.columns[test_matrix_hamming_df.columns != 'Lens ID']\n",
    "\n",
    "##### Hamming similarity rank matrix\n",
    "#### The list comprehension way!! Probs not apropriate \n",
    "rank_matrix_hamming_df = [pd.DataFrame(test_matrix_hamming_df[test_columns].rank(method = 'first', ascending = False)) for i in test_columns][1] \n",
    "\n",
    "lst = []\n",
    "rank_cols = test_matrix_hamming_df.columns\n",
    "\n",
    "rank_matrix_hamming_df['Lens ID'] = test_matrix_hamming_df.index\n",
    "\n",
    "for cols in rank_cols:\n",
    "    lst.append(rank_matrix_hamming_df[['Lens ID',cols]].loc[rank_matrix_hamming_df[cols] == 1])\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i]['new_index'] = str(i) + \"hello\"\n",
    "    lst[i].set_index(['new_index'], inplace = True)     \n",
    "\n",
    "lst2 = []\n",
    "for id,i in enumerate(lst):\n",
    "    if not i.empty:\n",
    "        lst2.append(i)\n",
    "    else:\n",
    "        pass   \n",
    "pd.concat(lst2)      \n",
    "\n",
    "rank_matrix_hamming_df = pd.concat(lst2).fillna(0)\n",
    "rank_matrix_hamming_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_hamming_df.csv\")\n",
    "\n",
    "\n",
    "########################################################\n",
    "######################### Jaccard ######################\n",
    "jaccard\n",
    "rank_matrix_jaccard_df = pd.DataFrame()\n",
    "rank_matrix_jaccard_df['Lens ID'] = test_matrix_jaccard_df.index\n",
    "rank_matrix_jaccard_df.index = test_matrix_jaccard_df.index\n",
    "test_columns = test_matrix_jaccard_df.columns[test_matrix_jaccard_df.columns != 'Lens ID']\n",
    "\n",
    "##### Hamming similarity rank matrix\n",
    "#### The list comprehension way!! Probs not apropriate \n",
    "rank_matrix_jaccard_df = [pd.DataFrame(test_matrix_jaccard_df[test_columns].rank(method = 'first', ascending = False)) for i in test_columns][1] \n",
    "\n",
    "lst = []\n",
    "rank_cols = test_matrix_jaccard_df.columns\n",
    "\n",
    "rank_matrix_jaccard_df['Lens ID'] = test_matrix_jaccard_df.index\n",
    "\n",
    "for cols in rank_cols:\n",
    "    lst.append(rank_matrix_jaccard_df[['Lens ID',cols]].loc[rank_matrix_jaccard_df[cols] == 1])\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    lst[i]['new_index'] = str(i) + \"hello\"\n",
    "    lst[i].set_index(['new_index'], inplace = True)     \n",
    "\n",
    "lst2 = []\n",
    "for id,i in enumerate(lst):\n",
    "    if not i.empty:\n",
    "        lst2.append(i)\n",
    "    else:\n",
    "        pass   \n",
    "pd.concat(lst2)      \n",
    "\n",
    "\n",
    "rank_matrix_jaccard_df = pd.concat(lst2).fillna(0)\n",
    "rank_matrix_jaccard_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_hamming_df.csv\")\n",
    "\n",
    "rank_matrix_jaccard_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "boring-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This might be important to present the data as a heatmap\n",
    "############# A web reference to building a heatmap for this cosine similarity matrix\n",
    "###https://blog.quantinsti.com/creating-heatmap-using-python-seaborn/\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "test_matrix_cosine_df\n",
    "\n",
    "#sb.heatmap(data)\n",
    "#np.array(test_matrix_cosine_df)[1]\n",
    "Lens_ID = test_matrix_cosine_df['Lens ID']\n",
    "np_df_matrix = test_matrix_cosine_df\n",
    "np_df_matrix['Lens ID']\n",
    "np_df_matrix =np_df_matrix.drop(columns = ['Lens ID'])\n",
    "#sb.heatmap(np_df_matrix.to_numpy())\n",
    "#plt.show()\n",
    "###### This heatmap is too big \n",
    "#ax = plt.subplots(figsize=(16,20)) \n",
    "\n",
    "#sb.heatmap(np_df_matrix.to_numpy()[0:30],ax=ax)\n",
    "#####\n",
    "\n",
    "def heatplot_func(partitions = 10,part = 0):\n",
    "    partitions = 10\n",
    "    lstty = []\n",
    "    for i in range(partitions):\n",
    "        lstty.append(285/partitions)\n",
    "#for id,i in enumerate(lstty):\n",
    "    \n",
    "    indexing2 = np.round(np.cumsum(lstty),0) \n",
    "    indexing1 = np.round(np.cumsum(lstty) - (285/partitions))   \n",
    "\n",
    "    ax = plt.subplots(figsize=(17,20))     \n",
    "    ax = sb.heatmap(np_df_matrix.to_numpy()[int(indexing1[part]):int(indexing2[part])], yticklabels=Lens_ID[int(indexing1[part]):int(indexing2[part])] ,xticklabels = test_matrix_cosine_df.columns[test_matrix_cosine_df.columns != 'Lens ID'])\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-roberts",
   "metadata": {},
   "source": [
    "### Heatmap of the pairwise similarity score for train rows of documents (Based on Cosine Similarity Scores)\n",
    "* The function below only visualises a portion of the train set as visualising all 285 rows too big!\n",
    "* x Axis represents the test set documents (72 documents)\n",
    "* y Axis represents the small subset of train set documents (28 rows at a time depending on what you set the partition argument in the heatplot_func below) \n",
    "* this function can be used again to visualise the next 9 partitions of the data by the 72 columns (represented in the X axis in the heat map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atlantic-campus",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'heatplot_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2569c183e97f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mheatplot_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#### Data validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#len(np_df_matrix.to_numpy()[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#len(np_df_matrix.to_numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'heatplot_func' is not defined"
     ]
    }
   ],
   "source": [
    "heatplot_func(partitions = 10,part = 0)\n",
    "\n",
    "#### Data validation\n",
    "#len(np_df_matrix.to_numpy()[1])\n",
    "#len(np_df_matrix.to_numpy())\n",
    "#Lens_ID[26]\n",
    "#len(np_df_matrix.to_numpy()[0:int(np.round(285/10,0))])\n",
    "#np_df_matrix.to_numpy()[26]\n",
    "#cosine_similarity(x_train[x_train.index =='012-309-807-113-075'],x_test[x_test.index =='118-102-398-817-762'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
