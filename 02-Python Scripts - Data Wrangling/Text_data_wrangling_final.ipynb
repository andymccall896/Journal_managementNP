{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sustainable-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling package in Python \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Glob pythons filepath recognition \n",
    "import glob \n",
    "# Pythons package is the way in which it can interact with the operating system\n",
    "import os\n",
    "# natural language processing package \n",
    "import nltk \n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer   \n",
    "ps = PorterStemmer() \n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "##### Concatenate the excel files together\n",
    "print(glob.glob(\"/home/adam/*.txt\"))\n",
    "os.chdir(\"C:/Users/andym/Desktop/Datafancy/Journal_managementNP/01-Raw_data/all_data\")\n",
    "# get data file names\n",
    "path =r'C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\01-Raw_data\\all_data'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "### set an empty list?\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['file_name'] = filename\n",
    "    li.append(df)\n",
    "    \n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "dframe = pd.DataFrame(data = frame)\n",
    "dframe['Publication Year'] = dframe['Publication Year'].astype('int') \n",
    "dframetrim = dframe[dframe.Abstract.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "located-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "englishstopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "under-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract']\n",
    "\n",
    "for i in range(len(dframetrim['Abstract'])):\n",
    "    dframetrim['Abstract_trim_2'].values[i]  = re.sub('[^A-Za-z0-9]+', ' ', dframetrim['Abstract_trim_2'].values[i].lower()) ## Lower case and special characters removed\n",
    "    dframetrim['Abstract_trim_2'].values[i]  = dframetrim['Abstract_trim_2'].values[i].split(' ') ## tokenise\n",
    "##### I might need to look up lambda\n",
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract_trim_2'].apply(lambda x: [item for item in x if item not in stopwords.words('english')]) ## Stopwords\n",
    "dframetrim['Abstract_trim_2'] =  dframetrim['Abstract_trim_2'].apply(lambda x: [porter.stem(item) for item in x ]) ## Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cultural-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andym\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "word_remove_list = ['knowledg','classif','inform','use','www','good','','organ'\n",
    "                    ,'paper','subject','term','studi','develop','term','librari'\n",
    "                    ,'differ','index','approach','work','base','scienc','field'\n",
    "                    ,'user','structure','ko','www','present','ko','also','p',\n",
    "                    'two','one','provid','author','need','tag','within','way','well','includ',\n",
    "                   'part','three','may','2','e','ddc','1','3','4','5','6','7','8','9','10','li',\n",
    "                    'would','non','de','four','ir','thu','frbr','737','g',\n",
    "                    'etc','av','br','co','th','o','sg','nr','per','1087',\n",
    "                   'six','j','via','miss','r','et','dan','bia','niac','sch','17525900'\n",
    "                   'u','lcc','c','ii','la','n','dc','aat' ,'24','TRUE','keep','220','le','po','stw',\n",
    "                   'kdc','oto','gsc','gsp','k','pt','dr','go','100','cc','il','nt','280','b','l','v',\n",
    "                   '20','kr','sp','ave','se','12','0','23','33','157','iv','rd','am','mid','34','be','iii','fid'\n",
    "                    ,'lc','met','20th' ,'ste','axi','dsi','axi','200','230','223','hlt','sur','du','mesh'\n",
    "                    ,'lcsh','oclc','eu','atn','2nd','bc2','di','76','500','get','http','sdss','crg'\n",
    "                    ,'esp','sab','lay','11','37','cs','urg','17525000','ot','fo','hn','em','ncft','48',\n",
    "                    '40','168','dec','pc','fix','ccq','lrt','ic','fix','owe','120th','87','510','94','49',\n",
    "                    '6n','mit','rev','int','32','136','144','cr','119','148','wall','120','ckm','cd','rom',\n",
    "                    'ata','nest','sdoc','soli','istei','tct','shl','ana','asp','dk5','240','292','299','f','ég','222'\n",
    "                    ,'229','59','pin','sen','twelv','bbk','14','28','180','192','cle','150','ec','np','ifla','ey','dea',\n",
    "                    'cpcl','nl','601','178','215','iskoi','htm','i','ref','31','92','97','çmf','ibn','vo','inv','un','kop','cen','21st'\n",
    "                    ,'659','ph','229','26','va','oc','ufpr','80060000','ena','od','fu','3d','dog','ear','vi','saa','vi','62',\n",
    "                    'ed','43hing','kno','pubm','39','268','275','els','fac','nd','ek','oth','er','cep','ia','iti','38','114',\n",
    "                   '122','35','lam','liu','ru','16','403','41','311','318','ga','eas','gri','tgn','cut','off','add','lrm','cm','ric','collat',\n",
    "                    'stop','unlik','die','sdi','90','379','1a','1b','rda','riv','hj','pac','hoc','rid','rio','ssn','real',\n",
    "                   'ccp','iht','edm','today','lie','point','bcc']\n",
    "\n",
    "dframetrim['Abstract_trim_2'] = dframetrim['Abstract_trim_2'].apply(lambda x: [item for item in x if item not in word_remove_list]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cardiovascular-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_df(testing_loop):\n",
    "\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "\n",
    "    def getUniqueWords(allWords):\n",
    "        uniqueWords = [] \n",
    "        for i in allWords:\n",
    "            if not i in uniqueWords:\n",
    "                uniqueWords.append(i)\n",
    "        return uniqueWords\n",
    "\n",
    "    testing_loop_long = testing_loop\n",
    "    testing_loop_long['Abstract_trim_2_distinct'] = testing_loop_long['Abstract_trim_2']\n",
    "    for id,i in enumerate(testing_loop_long['Abstract_trim_2']):\n",
    "        testing_loop_long['Abstract_trim_2_distinct'].iloc[id] = getUniqueWords(i)\n",
    "        \n",
    "    testing_loop_long = testing_loop_long[testing_loop_long.astype(str)['Abstract_trim_2_distinct'] != '[]']\n",
    "    testing_loop = testing_loop[testing_loop.astype(str)['Abstract_trim_2_distinct'] != '[]']    \n",
    "\n",
    "    testing_loop['abstract_counts'] = [Counter(x).most_common() for x in testing_loop['Abstract_trim_2']]\n",
    "\n",
    "    testing_loop = testing_loop[testing_loop.astype(str)['abstract_counts'] != '[]']\n",
    "\n",
    "    testing_loop_long = testing_loop[['Lens ID','Abstract_trim_2_distinct','Abstract_trim_2']].explode('Abstract_trim_2_distinct')\n",
    "\n",
    "    testing_loop_long['abstract_counts_unlist'] = [val for sublist in testing_loop['abstract_counts'] for val in sublist]\n",
    "\n",
    "#pd.DataFrame(testing_loop['abstract_counts_unlist'])\n",
    "\n",
    "#### Unzip the individual tuples and covert into seperate columns\n",
    "#testing_loop_long['Word'] = testing_loop_long['Abstract_trim_2_distinct']\n",
    "#testing_loop_long['Count'] = testing_loop_long['Abstract_trim_2_distinct']\n",
    "    testing_loop_long['Word'], testing_loop_long['Count'] = zip(*testing_loop_long['abstract_counts_unlist'])\n",
    "    testing_loop_long['Binary_count'] = 1\n",
    "\n",
    "    testing_loop_long[\"Total_words_in_abstract\"] = testing_loop_long['Abstract_trim_2']\n",
    "    for id,i in enumerate(testing_loop_long['Abstract_trim_2']):\n",
    "        testing_loop_long[\"Total_words_in_abstract\"].iloc[id] = len(testing_loop_long['Abstract_trim_2'].iloc[id])\n",
    "    testing_loop_long['Cal_Term_Freq_TF'] =  (testing_loop_long['Count']/testing_loop_long[\"Total_words_in_abstract\"])\n",
    "    \n",
    "    testing_loop_long_Count = testing_loop_long[['Lens ID','Word','Count']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Count').fillna(0).add_prefix('Count_')   \n",
    "    testing_loop_long_Count = pd.DataFrame(testing_loop_long_Count.to_records())\n",
    "\n",
    "    testing_loop_long_TF = testing_loop_long[['Lens ID','Word','Cal_Term_Freq_TF']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Cal_Term_Freq_TF',aggfunc = np.sum).fillna(0).add_prefix('TF_')      \n",
    "    testing_loop_long_TF = pd.DataFrame(testing_loop_long_TF.to_records()) \n",
    "\n",
    "    testing_loop_long_Binary = testing_loop_long[['Lens ID','Word','Binary_count']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Binary_count').fillna(0).add_prefix('Binary_') \n",
    "    testing_loop_long_Binary = pd.DataFrame(testing_loop_long_Binary.to_records()) \n",
    "\n",
    "    testing_loop_long['abstract_concat_list'] = testing_loop_long['Abstract_trim_2']\n",
    "    for i in range(len(testing_loop_long['Abstract_trim_2'])):\n",
    "        testing_loop_long['abstract_concat_list'].values[i] = list(testing_loop['Abstract_trim_2'])\n",
    "    \n",
    "    testing_loop_long['dsad'] = testing_loop_long['abstract_concat_list']\n",
    "    for id1,o in enumerate(testing_loop_long['abstract_concat_list']):\n",
    "        for id2,i in enumerate(o):\n",
    "            testing_loop_long['dsad'].iloc[id1][id2] = [i for i in testing_loop_long['abstract_concat_list'].iloc[id1][id2] if i == testing_loop_long['Word'].values[id1]] \n",
    "\n",
    "    for id,i in enumerate(testing_loop_long['dsad']):\n",
    "        testing_loop_long['dsad'].values[id] = [x for x in testing_loop_long['dsad'].values[id] if x != []]        \n",
    "\n",
    "    testing_loop_long['times_mentioned_in_abstracts'] = [len(x) for x in testing_loop_long['dsad']]   \n",
    "    testing_loop_long['Total_number_abstracts'] = len(testing_loop[\"Abstract_trim_2\"])       \n",
    "    testing_loop_long['Cal_Inverse_Document_Freq_IDF'] = np.log10(testing_loop_long['Total_number_abstracts']/testing_loop_long['times_mentioned_in_abstracts'])\n",
    "\n",
    "    testing_loop_long_IDF = testing_loop_long[['Lens ID','Word','Cal_Inverse_Document_Freq_IDF']].pivot_table(index = 'Lens ID',columns = 'Word' , values = 'Cal_Inverse_Document_Freq_IDF').fillna(0).add_prefix('IDF_')  \n",
    "    testing_loop_long_IDF = pd.DataFrame(testing_loop_long_IDF.to_records()) \n",
    "\n",
    "    testing_loop_wide = testing_loop_long_IDF.set_index('Lens ID').join(testing_loop_long_TF.set_index('Lens ID')).join(testing_loop_long_Count.set_index('Lens ID')).join(testing_loop_long_Binary.set_index('Lens ID'))\n",
    "\n",
    "    return testing_loop_wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "speaking-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## If you need to re-create the word_count_df\n",
    "#testsmall = word_count_df(testing_loop =  dframetrim.iloc[0:2])\n",
    "#test = word_count_df(testing_loop =  dframetrim)\n",
    "#test.to_csv(r'C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\Final_pivot_dataset_abstract.csv')\n",
    "\n",
    "Final_pivot_dataset = pd.read_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\Final_pivot_dataset_abstract.csv\")\n",
    "Final_pivot_dataset = Final_pivot_dataset.set_index(\"Lens ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "normal-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split data sets randomly from 80% to 20% and do a iterative loop to of comparing each \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Final_pivot_dataset_IDF = Final_pivot_dataset.loc[:, Final_pivot_dataset.columns.str.startswith(\"IDF_\")]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train ,x_test = train_test_split(Final_pivot_dataset_IDF,test_size=0.2,random_state  = 11) \n",
    "x_test\n",
    "\n",
    "###########################################################################################\n",
    "#################### COSINE SIMILARITY ####################################################\n",
    "###########################################################################################\n",
    "\n",
    "#### Needs spot checking\n",
    "test_matrix_cosine = cosine_similarity(x_train,x_test) ### I think this does work\n",
    "test_matrix_cosine.shape\n",
    "\n",
    "#pd.DataFrame(test_matrix_cosine).to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix.csv\")\n",
    "test_matrix_cosine_df = pd.DataFrame(test_matrix_cosine)\n",
    "test_matrix_cosine_df = test_matrix_cosine_df.set_index(x_train.index)\n",
    "test_matrix_cosine_df.columns = x_test.index\n",
    "test_matrix_cosine_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_cosine.csv\")\n",
    "\n",
    "############################################################################################\n",
    "#################### HAMMING DISTANCE ######################################################\n",
    "############################################################################################\n",
    "from scipy.spatial import distance\n",
    "\n",
    "lst2 = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    lst2.append(i)\n",
    "\n",
    "lst = [[] for _ in range(len(x_train))]\n",
    "test_matrix_hamming =[lst2 for i in lst]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_test)):\n",
    "        test_matrix_hamming[i][j] = distance.hamming(x_train[i:i+1],x_test[j:j+1])        \n",
    "        \n",
    "test_matrix_hamming_df = pd.DataFrame(test_matrix_hamming)\n",
    "test_matrix_hamming_df = test_matrix_hamming_df.set_index(x_train.index)\n",
    "test_matrix_hamming_df.columns = x_test.index\n",
    "test_matrix_hamming_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_hamming.csv\")\n",
    "############################################################################################\n",
    "#################### JACCARD DISTANCE ######################################################\n",
    "############################################################################################\n",
    "from scipy.spatial import distance\n",
    "distance.jaccard\n",
    "\n",
    "lst2 = []\n",
    "for i in range(len(x_test)):\n",
    "    lst2.append(i)\n",
    "\n",
    "lst = [[] for _ in range(len(x_train))]\n",
    "test_matrix_jaccard =[lst2 for i in lst]\n",
    "\n",
    "for i in range(len(x_train)):\n",
    "    for j in range(len(x_test)):\n",
    "        test_matrix_jaccard[i][j] = distance.jaccard(x_train[i:i+1],x_test[j:j+1])    \n",
    "                \n",
    "test_matrix_jaccard_df = pd.DataFrame(test_matrix_jaccard)\n",
    "test_matrix_jaccard_df = test_matrix_jaccard_df.set_index(x_train.index)\n",
    "test_matrix_jaccard_df.columns = x_test.index\n",
    "test_matrix_jaccard_df.to_csv(r\"C:\\Users\\andym\\Desktop\\Datafancy\\Journal_managementNP\\03-test_datasets\\test_matrix_jaccard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eleven-small",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65666894]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### spot check each Similarity matrix\n",
    "\n",
    "x_train.columns\n",
    "x_test.columns\n",
    "x_train.index\n",
    "#103-364-820-610-122\n",
    "#129-504-756-103-235\n",
    "x_test.index\n",
    "#000-251-444-646-256\n",
    "#114-672-389-593-030\n",
    "\n",
    "###### Cosine Similarity test \n",
    "test_matrix_cosine_df\n",
    "cosine_similarity(x_train[x_train.index =='103-364-820-610-122'],x_test[x_test.index =='000-251-444-646-256'])\n",
    "\n",
    "##### High Similarity (These articles had the same author?)\n",
    "cosine_similarity(x_train[x_train.index =='129-504-756-103-235'],x_test[x_test.index =='114-672-389-593-030'])\n",
    "cosine_similarity(x_train[x_train.index =='129-504-756-103-235'],x_test[x_test.index =='166-898-814-966-01X'])\n",
    "cosine_similarity(x_train[x_train.index =='048-107-503-236-974'],x_test[x_test.index =='001-281-947-751-975'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "divided-spirituality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This might be important to present the data as a heatmap\n",
    "import seaborn as sb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
